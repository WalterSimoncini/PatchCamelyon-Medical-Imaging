{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c110e1-5edb-4044-a3b2-b9d85d4f5390",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import torch\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from typing import Tuple, List\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.models import resnet18, ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac465f-3ec3-410e-83c3-e7d4387da857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trick needed to make logging work in an IPython notebook\n",
    "# https://stackoverflow.com/a/21475297\n",
    "from importlib import reload\n",
    "reload(logging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cdf682-be25-46e3-9a75-a506c6db2a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1359f7-a0cc-418f-b356-b9c9e60a80d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = \"runs\"\n",
    "# This will only use ~7GBs of VRAM, but maybe we should use a lower number?\n",
    "# As far as I know a smaller batch size leads to better generalization\n",
    "BATCH_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb99499-b0f1-4462-8813-e210fe19cbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c0036-f4ad-48c9-adac-b781e9334b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "  format=\"[%(asctime)s:%(levelname)s]: %(message)s\",\n",
    "  level=logging.INFO\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: set all hyperparameters here, or use a config file\n",
    "# that way we can keep track of all the hyperparameters we used in each run on wandb\n",
    "\n",
    "config = {'BATCH_SIZE': BATCH_SIZE, \n",
    "          'EPOCHS': 100,\n",
    "          'LEARNING_RATE': 0.001,\n",
    "          'WEIGHT_DECAY': 0.0001,\n",
    "          'MOMENTUM': 0.9,\n",
    "          'NUM_WORKERS': 4,\n",
    "          'DEVICE': 'cuda',\n",
    "          }\n",
    "\n",
    "# WandB: Define metadata of the run\n",
    "run_name = 'test'\n",
    "notes = 'Lorem ipsum dolor sit amet, consectetur adipiscing elit.'\n",
    "tags = ['tag1', 'tag2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e00e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wandb\n",
    "import wandb\n",
    "wandb.login()\n",
    "%env \"WANDB_NOTEBOOK_NAME\" \"train.ipynb\"\n",
    "%env WANDB_SILENT=False\n",
    "\n",
    "run = wandb.init(name=run_name, notes = notes, tags = tags, project='PatchCamelyon',  entity='mi_ams',  config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7b96d8-c4c4-4eb0-b56e-0d61bcc4dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchCamelyonDataset(Dataset):\n",
    "    def __init__(self, data_path: str, targets_path: str, transform=None) -> None:\n",
    "        self.data = h5py.File(data_path)#[\"x\"]\n",
    "        self.targets = h5py.File(targets_path)#[\"y\"]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.targets.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.tensor, torch.tensor]:\n",
    "        sample = torch.tensor(self.data[idx, :, :, :]).float() / 255.0\n",
    "        # [channels, x, y] to [x, y, channels]\n",
    "        sample = torch.permute(sample, (2, 0, 1))\n",
    "\n",
    "        # We need to squeeze the targets as they are\n",
    "        # nested within multiple arrays\n",
    "        target = torch.tensor(self.targets[idx].squeeze())\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5912da31-865f-481a-a359-d1241a55d31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(imgs: List[torch.tensor] | torch.tensor, labels: List[str | int] = None):    \n",
    "    if type(imgs) != list:\n",
    "        imgs = [imgs]\n",
    "\n",
    "    if labels is None:\n",
    "        labels = [\"\"] * len(imgs)\n",
    "\n",
    "    fig, axs = plt.subplots(ncols=len(imgs), squeeze=False, figsize=(12, 6))\n",
    "\n",
    "    for i, (img, label) in enumerate(zip(imgs, labels)):\n",
    "        img = img.detach()\n",
    "        img = F.to_pil_image(img)\n",
    "\n",
    "        axs[0, i].imshow(np.asarray(img))\n",
    "        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])\n",
    "        axs[0, i].set_xlabel(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021c5501-b85f-4582-88e1-27ca42eeb282",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PatchCamelyonDataset(\n",
    "    data_path=\"data/camelyonpatch_level_2_split_train_x.h5\",\n",
    "    targets_path=\"data/camelyonpatch_level_2_split_train_y.h5\",\n",
    "    transform=transforms.Resize(224, antialias=True)\n",
    ")\n",
    "\n",
    "val_dataset = PatchCamelyonDataset(\n",
    "    data_path=\"data/camelyonpatch_level_2_split_valid_x.h5\",\n",
    "    targets_path=\"data/camelyonpatch_level_2_split_valid_y.h5\",\n",
    "    transform=transforms.Resize(224, antialias=True)\n",
    ")\n",
    "\n",
    "train_dataset = PatchCamelyonDataset(\n",
    "    data_path=\"data/camelyonpatch_level_2_split_test_x.h5\",\n",
    "    targets_path=\"data/camelyonpatch_level_2_split_test_y.h5\",\n",
    "    transform=transforms.Resize(224, antialias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b1d88-0cc0-4055-b4fc-9dd9ae16790c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class2label = {\n",
    "    0: \"No Tumor\",\n",
    "    1: \"Tumor\"\n",
    "}\n",
    "\n",
    "samples, labels = zip(*[train_dataset[x] for x in range(5)])\n",
    "samples, labels = list(samples), [class2label[x.item()] for x in list(labels)]\n",
    "\n",
    "show(samples, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa8417f-462b-40f7-9f32-07e1507aad9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    train_loader: DataLoader,\n",
    "    optimizer: optim.Optimizer,\n",
    "    loss_fn: nn.Module\n",
    "):\n",
    "    model.train()\n",
    "    train_loss, correct_preds, batches_n = 0, 0, 0\n",
    "\n",
    "    for _, (images, targets) in enumerate(tqdm(train_loader)):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(images)\n",
    "\n",
    "        loss = loss_fn(preds, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Record the training loss and the number of correct predictions\n",
    "        batches_n += 1\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        preds = preds.argmax(dim=1)\n",
    "        correct_preds += (preds == targets).sum()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "    train_loss /= batches_n\n",
    "    accuracy = correct_preds / len(train_loader.dataset)\n",
    "\n",
    "    return train_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b459925-a9b7-4158-9ca6-0a52c1f84655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(\n",
    "    model: nn.Module,\n",
    "    device: torch.device,\n",
    "    test_loader: DataLoader\n",
    "):\n",
    "    model.eval()\n",
    "    test_loss, correct_preds, batches_n = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _, (images, targets) in enumerate(tqdm(test_loader)):\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            preds = model(images)\n",
    "            test_loss += loss_fn(preds, targets)\n",
    "\n",
    "            batches_n += 1\n",
    "            preds = preds.argmax(dim=1)\n",
    "            correct_preds += (preds == targets).sum()\n",
    "\n",
    "    test_loss /= batches_n\n",
    "    accuracy = correct_preds / len(test_loader.dataset)\n",
    "\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf24a22-4f25-4d9b-bfcb-caa972476830",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9658017f-a3c8-4ff2-b080-69108d7f1ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8107bb-bf87-4732-88eb-305e3e882927",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(in_features=512, out_features=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8a02de-9f92-40ac-b369-df98b2fc20ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7242da5-40a9-42df-92c2-5e9bc99dca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%wandb\n",
    "\n",
    "best_val_accuracy = 0\n",
    "run_id = datetime.utcnow().strftime(\"%Y-%m-%dT%H%M%S\")\n",
    "run_folder = os.path.join(OUTPUT_PATH, run_id)\n",
    "\n",
    "os.makedirs(run_folder)\n",
    "writer = SummaryWriter(run_folder)\n",
    "\n",
    "for epoch in range(10):\n",
    "    logging.info(f\"starting epoch {epoch}\")\n",
    "\n",
    "    train_loss, train_accuracy = train_epoch(\n",
    "        model=model,\n",
    "        device=device,\n",
    "        optimizer=optimizer,\n",
    "        loss_fn=loss_fn,\n",
    "        train_loader=train_loader\n",
    "    )\n",
    "\n",
    "    logging.info(f\"the train accuracy was {train_accuracy} (loss: {train_loss})\")\n",
    "\n",
    "    val_loss, val_accuracy = test(model=model, device=device, test_loader=val_loader)\n",
    "\n",
    "    logging.info(f\"the validation accuracy was {val_accuracy} (loss: {val_loss})\")\n",
    "\n",
    "    # Log metrics to tensorboard\n",
    "    writer.add_scalar(\"loss/train\", train_loss, epoch)\n",
    "    writer.add_scalar(\"loss/val\", val_loss, epoch)\n",
    "\n",
    "    writer.add_scalar(\"acc/train\", train_accuracy, epoch)\n",
    "    writer.add_scalar(\"acc/val\", val_accuracy, epoch)\n",
    "    \n",
    "    # Log epoch metrics\n",
    "    wandb.log(\n",
    "        {\n",
    "        \"Epoch\": epoch,\n",
    "        \"loss/train\": train_loss,\n",
    "        \"loss/val\": val_loss,\n",
    "        \"acc/train\": train_accuracy,\n",
    "        \"acc/val\": val_accuracy,\n",
    "        }\n",
    "    )\n",
    "       \n",
    "    # Pick the best model according to the validation\n",
    "    # accuracy score\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        logging.info(f\"found new best model at epoch {epoch} with accuracy {val_accuracy} (loss {val_loss})\")\n",
    "\n",
    "        best_val_accuracy = val_accuracy\n",
    "        wandb.run.summary[\"best_val_accuracy\"] = best_val_accuracy\n",
    "        wandb.run.summary[\"best_val_epoch\"] = epoch\n",
    "\n",
    "        # Save the model to disk\n",
    "        torch.save(\n",
    "            model.state_dict(),\n",
    "            os.path.join(run_folder, f\"model_{epoch}.pt\")\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344e48a8-c0d1-494d-a1a9-34480f6703e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = test(model=model, device=device, test_loader=test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d764fd93-f254-4872-8d0d-ebf2342711e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.info(f\"the test accuracy was {test_accuracy}\")\n",
    "wandb.run.summary[\"acc/test\"] = test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdae89c-8da8-45ad-99cd-64427ad744c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ca3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tell wandb we are done with this notebook\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
